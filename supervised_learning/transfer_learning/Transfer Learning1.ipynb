{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9f48323",
   "metadata": {},
   "source": [
    "# Transfer Learning on Cifar-10 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5655c274",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The CIFAR-10 dataset is a popular standard for evaluating image classification algorithms. It comprises 60,000 color images, each with a resolution of 32x32 pixels, divided equally into 10 categories. Each category contains 6,000 images and includes the following classes: airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks.\n",
    "\n",
    "These low-resolution images present classification challenges due to their small size and the diversity of their appearances. CIFAR-10 is frequently utilized to assess the effectiveness of various image classification methods, especially those involving deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d5cb42",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d788a4cc",
   "metadata": {},
   "source": [
    "### Preparation: Loading CIFAR-10\n",
    "\n",
    "The CIFAR-10 dataset includes 60,000 color images at a resolution of 32x32 pixels, divided into 10 categories, each with 6,000 images. The dataset is separated into two parts:\n",
    "\n",
    "- 50,000 images for training\n",
    "- 10,000 images for testing\n",
    "\n",
    "The libraries used, such as TensorFlow and Keras, provide essential tools for efficiently developing and training neural network models. TensorFlow acts as the core framework for creating computational graphs and performing machine learning tasks, while Keras offers a high-level API to simplify the construction and training of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160f01db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 13:00:21.676422: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "# Main imports needed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d155f10",
   "metadata": {},
   "source": [
    "# 2. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e43b458",
   "metadata": {},
   "source": [
    "The next step is to load the CIFAR-10 dataset using Keras' built-in `cifar10.load_data()` function. This dataset contains 60,000 color images with a resolution of 32x32 pixels, divided into 10 categories, each comprising 6,000 images. The images are partitioned into 50,000 for training and 10,000 for testing. After loading the dataset, the pixel values are converted to floating-point numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bba2adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (50000, 32, 32, 3)\n",
      "Test data shape (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "# Using keras\n",
    "\n",
    "(x_train_full, y_train_full), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "print(\"Training data shape\", x_train_full.shape)\n",
    "print(\"Test data shape\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a77499",
   "metadata": {},
   "source": [
    "# 3. Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a6ede",
   "metadata": {},
   "source": [
    "Lets get some insight into the dataset, enabling better understanding and decision-making throughout the model adaptation process. Visualizing data helps identify patterns, anomalies, and distributions, ensuring the pre-trained model's assumptions align with the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c264ec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Create a new figure\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Loop over the first 24 images\n",
    "for i in range(24):\n",
    "    # Create a subplot for each image\n",
    "    plt.subplot(4, 6, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(x_train_full[i])\n",
    "\n",
    "    # Set the label as the title\n",
    "    plt.title(class_names[y_train_full[i][0]], fontsize=12)\n",
    "\n",
    "# Display the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a76be7c",
   "metadata": {},
   "source": [
    "# 4. Build Transfer Learning Model\n",
    "\n",
    "## 4.1 Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b724648",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Flatten, UpSampling2D\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5205057b",
   "metadata": {},
   "source": [
    "## 4.2 Preprocess Input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7718cca",
   "metadata": {},
   "source": [
    "\n",
    "In this step, we use the `preprocess_input` function to prepare our input data. This preprocessing is crucial when utilizing pre-trained convolutional neural network (CNN) models such as ResNet50, VGG16, or InceptionV3. It ensures that our input images are properly formatted and normalized before being used for training or inference.\n",
    "\n",
    "Key aspects of `preprocess_input` include:\n",
    "\n",
    "- Standardizing input data to meet the requirements of pre-trained CNN models.\n",
    "- Performing mean normalization and channel-wise color normalization.\n",
    "- Scaling and centering input images to enhance convergence during training and improve accuracy during inference.\n",
    "- Maintaining numerical stability to prevent issues like vanishing or exploding gradients.\n",
    "- Improving the model's generalization by ensuring consistent and standardized input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42942171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (50000, 32, 32, 3)\n",
      "Test data shape: (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train_full = x_train_full.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Assuming x_train_full and x_test are already loaded as numpy arrays\n",
    "x_train_full = preprocess_input(x_train_full)\n",
    "x_test = preprocess_input(x_test)\n",
    "\n",
    "print(\"Training data shape:\", x_train_full.shape)\n",
    "print(\"Test data shape:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bcb48a4",
   "metadata": {},
   "source": [
    "## 4.3 Train, Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914903ed",
   "metadata": {},
   "source": [
    "\n",
    "In this step, we divide the training set into separate training and validation sets. The full training set, `x_train_full`, is split as follows:\n",
    "\n",
    "- `x_train` contains most of the data.\n",
    "- `x_valid` holds a smaller portion (5,000 samples) for validation.\n",
    "\n",
    "The labels are similarly split into `y_train` and `y_valid`.\n",
    "\n",
    "Additionally, we convert the class labels from integers to categorical format using the `to_categorical` function. This conversion, necessary for categorical classification tasks like CIFAR-10, ensures that labels are represented as one-hot vectors for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ade95a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape (45000, 32, 32, 3)\n",
      "Test data shape (10000, 32, 32, 3)\n",
      "Valid data shape (5000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_valid = x_train_full[:-5000], x_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_valid = to_categorical(y_valid, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "print(\"Training data shape\", x_train.shape)\n",
    "print(\"Test data shape\", x_test.shape)\n",
    "print(\"Valid data shape\", x_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e46fb80",
   "metadata": {},
   "source": [
    "## 4.4 Define Feature Extractor and Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7923c776",
   "metadata": {},
   "source": [
    "In this step, we create a classifier function to build the classification layers on top of features extracted by ResNet50. The classifier maps these features to class probabilities.\n",
    "\n",
    "The function starts with global average pooling to condense the feature maps, then flattens them into a 1D vector. It adds two dense layers with ReLU activation for non-linearity and pattern learning. Finally, a dense layer with softmax activation outputs probabilities for the 10 CIFAR-10 classes, normalizing the results to sum up to 1. The output layer is named \"classification\" for easy identification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5441c208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature extractor using ResNet50\n",
    "def feature_extractor(inputs):\n",
    "    base_model = tf.keras.applications.ResNet50(\n",
    "        input_shape=(224, 224, 3), include_top=False, weights='imagenet')\n",
    "\n",
    "    # Freeze the layers of the base model\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "        return base_model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4751c149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(inputs):\n",
    "    x = GlobalAveragePooling2D()(inputs)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(10, activation='softmax', name=\"classification\")(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e93df",
   "metadata": {},
   "source": [
    "## Defining the Final Model\n",
    "In this step, we combine the feature extraction and classification components to create the final model, which takes image inputs and produces output predictions.\n",
    "\n",
    "The `final_model` function begins by upsampling the input images using the `UpSampling2D` layer to increase their size to (224, 224), matching the input size required by ResNet50. The resized images are then processed by the feature extractor, which uses the pre-trained ResNet50 model to extract meaningful features.\n",
    "\n",
    "These features are passed to the classifier, which consists of several dense layers and a softmax output layer. The classifier converts the extracted features into class probabilities, indicating the likelihood of each input image belonging to each predefined class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e31fc110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model(inputs):\n",
    "    resize = UpSampling2D(size=(7,7))(inputs)\n",
    "    resnet_fe = feature_extractor(resize)\n",
    "    classification_output = classifier(resnet_fe)\n",
    "    \n",
    "    return classification_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a833b25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model():\n",
    "    inputs = Input(shape=(32, 32,3))\n",
    "    classification_output = final_model(inputs)\n",
    "    model = Model(inputs=inputs, outputs=classification_output)\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9dde7c",
   "metadata": {},
   "source": [
    "## Creating and Summarizing the Model\n",
    "\n",
    "In this step, we build the neural network model, specifying its architecture and compiling it with chosen optimization parameters, loss function, and evaluation metrics. After creation, we use the summary method to print a concise overview of the model's structure, detailing each layer's type, shape, number of parameters, and output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e057823e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">UpSampling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,098,176</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,800</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ classification (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">5,130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m3\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ up_sampling2d (\u001b[38;5;33mUpSampling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │    \u001b[38;5;34m23,587,712\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │     \u001b[38;5;34m2,098,176\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m524,800\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ classification (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m5,130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,215,818</span> (100.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,215,818\u001b[0m (100.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,162,698</span> (99.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,162,698\u001b[0m (99.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,120</span> (207.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m53,120\u001b[0m (207.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = compile_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ee784d",
   "metadata": {},
   "source": [
    "## Training the Model with Early Stopping\n",
    "\n",
    "We use the early stopping technique by setting up an early stopping callback to monitor the validation loss during training. If the validation loss doesn't improve for a specified number of epochs (patience), training stops. The `restore_best_weights=True` argument ensures the model reverts to the weights that achieved the lowest validation loss at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fb5faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m   6/1407\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:37:11\u001b[0m 9s/step - accuracy: 0.1647 - loss: 3.2323"
     ]
    }
   ],
   "source": [
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(x_train, y_train, epochs=20, batch_size=32, validation_data=(x_valid, y_valid), callbacks=[early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
